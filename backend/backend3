from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict
from datetime import datetime, timedelta
import json, base64, io, os
from openai import OpenAI
from gtts import gTTS
import requests
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
import pytz

# --------------------------------
# 환경 설정
# --------------------------------
kst = pytz.timezone('Asia/Seoul')
TOKEN_FILE = 'token.json'
CREDS_FILE = 'credentials.json'

openai = OpenAI(api_key="")
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

active_conns: Dict[str, WebSocket] = {}

# --------------------------------
# Function Calling 기능 정의
# --------------------------------
functions = [
    {
        "type": "function",
        "function": {
            "name": "generate_image",
            "description": "이미지 생성",
            "parameters": {
                "type": "object",
                "properties": {
                    "prompt": {"type": "string", "description": "이미지 설명"}
                },
                "required": ["prompt"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "text_to_speech",
            "description": "텍스트를 음성으로 변환",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "읽을 텍스트"}
                },
                "required": ["text"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "speech_to_text",
            "description": "음성을 텍스트로 변환",
            "parameters": {
                "type": "object",
                "properties": {
                    "audio_base64": {"type": "string", "description": "base64 인코딩된 음성"}
                },
                "required": ["audio_base64"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "talk_with_gpt",
            "description": "음성을 텍스트로 바꾸고 GPT가 대답한 결과를 음성으로 변환",
            "parameters": {
                "type": "object",
                "properties": {
                    "audio_base64": {"type": "string", "description": "base64 인코딩된 음성"}
                },
                "required": ["audio_base64"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "add_calendar_event",
            "description": "구글 캘린더에 일정 등록",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "datetime": {"type": "string", "format": "date-time"}
                },
                "required": ["title", "datetime"]
            }
        }
    }
]

# --------------------------------
# 기능 함수
# --------------------------------
def call_function(name: str, args: dict):
    if name == "generate_image":
        resp = openai.images.generate(
            model="dall-e-3",
            prompt=args["prompt"],
            n=1,
            size="1024x1024"
        )
        img_url = resp.data[0].url
        return {
            "type": "image",
            "message": args["prompt"],
            "imageData": base64.b64encode(requests.get(img_url).content).decode()
        }

    elif name == "text_to_speech":
        tts = gTTS(text=args["text"], lang="ko")
        buf = io.BytesIO()
        tts.write_to_fp(buf)
        return {
            "type": "audio",
            "message": args["text"],
            "audioData": base64.b64encode(buf.getvalue()).decode()
        }

    elif name == "speech_to_text":
        audio = base64.b64decode(args["audio_base64"])
        resp = openai.audio.transcriptions.create(
            model="whisper-1",
            file=io.BytesIO(audio),
            language="ko"
        )
        return {"type": "text", "message": resp.text}

    elif name == "talk_with_gpt":
        audio = base64.b64decode(args["audio_base64"])
        transcript = openai.audio.transcriptions.create(
            model="whisper-1",
            file=io.BytesIO(audio),
            language="ko"
        ).text

        chat = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": transcript}]
        )
        text = chat.choices[0].message.content
        tts = gTTS(text=text, lang="ko")
        buf = io.BytesIO()
        tts.write_to_fp(buf)

        return {
            "type": "text+audio",
            "message": text,
            "audioData": base64.b64encode(buf.getvalue()).decode()
        }

    elif name == "add_calendar_event":
        title = args["title"]
        dt_str = args["datetime"]
        dt = datetime.fromisoformat(dt_str).astimezone(kst)
        now = datetime.now(kst)

        # ✅ 과거 시점이면 1시간 뒤로 자동 보정
        if dt < now:
            dt = now + timedelta(hours=1)

        if os.path.exists(TOKEN_FILE):
            creds = Credentials.from_authorized_user_file(TOKEN_FILE)
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CREDS_FILE, scopes=["https://www.googleapis.com/auth/calendar"])
            creds = flow.run_local_server(port=0)
            with open(TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())

        service = build("calendar", "v3", credentials=creds)
        event = {
            "summary": title,
            "start": {"dateTime": dt.isoformat(), "timeZone": "Asia/Seoul"},
            "end": {"dateTime": (dt + timedelta(hours=1)).isoformat(), "timeZone": "Asia/Seoul"},
        }
        service.events().insert(calendarId="primary", body=event).execute()

        return {
            "type": "text",
            "message": f"✅ 일정 등록 완료: {title} — {dt.strftime('%Y-%m-%d %H:%M')}"
        }

    return {"type": "text", "message": f"⚠️ 알 수 없는 함수: {name}"}

# --------------------------------
# WebSocket 처리
# --------------------------------
@app.websocket("/ws/{nickname}")
async def ws_endpoint(ws: WebSocket, nickname: str):
    await ws.accept()
    active_conns[nickname] = ws
    try:
        while True:
            raw = await ws.receive_text()
            data = json.loads(raw)
            user_msg = data.get("message", "")
            audio_data = data.get("audioData")
            image_data = data.get("imageData")

            resp = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": user_msg}],
                tools=functions,
                tool_choice="auto"
            )

            msg = resp.choices[0].message
            if msg.tool_calls:
                call = msg.tool_calls[0]
                fname = call.function.name
                args = json.loads(call.function.arguments)

                # base64 삽입 보조
                if "audio_base64" in args and not args["audio_base64"]:
                    args["audio_base64"] = audio_data

                result = call_function(fname, args)
                result.update({"nickname": "GPT", "timestamp": datetime.now().isoformat()})
                await ws.send_text(json.dumps(result))
            else:
                reply = msg.content
                await ws.send_text(json.dumps({
                    "type": "text",
                    "nickname": "GPT",
                    "message": reply,
                    "timestamp": datetime.now().isoformat()
                }))

    except WebSocketDisconnect:
        active_conns.pop(nickname, None)

# --------------------------------
# 서버 실행
# --------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("backend3:app", host="0.0.0.0", port=8000, reload=True)

